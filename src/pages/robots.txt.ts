// src/pages/robots.txt.ts
import type { APIRoute } from 'astro';
// ⬇️ Change the path/name if yours is exported as `siteunctions`
import { siteFunctions } from '@/config/siteFunctions';

const AI_AGENTS = [
  'GPTBot',
  'CCBot',
  'ClaudeBot',
  'anthropic-ai',
  'PerplexityBot',
  'Google-Extended',
  'FacebookBot',
  'Applebot-Extended',
];

export const GET: APIRoute = ({ site }) => {
  const isProd = import.meta.env.PROD;
  const cfg = siteFunctions.robots;
  const sitemapHref = site ? new URL(cfg.sitemapUrl, site).href : cfg.sitemapUrl;

  // Block everything on non-prod if enabled
  if (!isProd && cfg.blockAllInNonProd) {
    const body = `User-agent: *
Disallow: /
Sitemap: ${sitemapHref}
# non-production block`;
    return new Response(body, {
      headers: { 'Content-Type': 'text/plain; charset=utf-8' },
    });
  }

  // Production robots
  const lines: string[] = [
    '# robots.txt generated by NViewsWeb',
    'User-agent: *',
    ...cfg.disallow.map((p) => `Disallow: ${p}`),
  ];

  if (cfg.blockAICrawlers) {
    lines.push('', '# AI crawlers blocked');
    for (const agent of AI_AGENTS) {
      lines.push(`User-agent: ${agent}`, 'Disallow: /', '');
    }
  }

  lines.push(`Sitemap: ${sitemapHref}`);

  return new Response(lines.join('\n'), {
    headers: { 'Content-Type': 'text/plain; charset=utf-8' },
  });
};
